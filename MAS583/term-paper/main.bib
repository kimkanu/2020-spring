
@InProceedings{pennington-bahri,
  title = 	 {Geometry of Neural Network Loss Surfaces via Random Matrix Theory},
  author = 	 {Jeffrey Pennington and Yasaman Bahri},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2798--2806},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/pennington17a.html},
  abstract = 	 {Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, $\phi$, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function $1/2(1-\phi)^2$.}
}
@incollection{pennington-worah1,
title = {Nonlinear random matrix theory for deep learning},
author = {Pennington, Jeffrey and Worah, Pratik},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {2637--2646},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6857-nonlinear-random-matrix-theory-for-deep-learning.pdf}
}
@incollection{pennington-worah2,
title = {The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network},
author = {Pennington, Jeffrey and Worah, Pratik},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {5410--5419},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7786-the-spectrum-of-the-fisher-information-matrix-of-a-single-hidden-layer-neural-network.pdf}
}
@article{choromanska,
  author    = {Anna Choromanska and
               Mikael Henaff and
               Micha{\"{e}}l Mathieu and
               G{\'{e}}rard Ben Arous and
               Yann LeCun},
  title     = {The Loss Surface of Multilayer Networks},
  journal   = {CoRR},
  volume    = {abs/1412.0233},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.0233},
  archivePrefix = {arXiv},
  eprint    = {1412.0233},
  timestamp = {Mon, 13 Aug 2018 16:48:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChoromanskaHMAL14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{dupic-castillo,
  title={Spectral density of products of Wishart dilute random matrices. Part I: the dense case},
  author={Thomas Dupic and Isaac P'erez Castillo},
  year={2014}
}
@article{speicher,
  title={Free Probability Theory},
  author={Roland Speicher},
  journal={Jahresbericht der Deutschen Mathematiker-Vereinigung},
  year={2009},
  volume={119},
  pages={3-30}
}